#! /usr/bin/env python
import os

from collections import defaultdict
from dataclasses import dataclass, field
from typing import List, Tuple

import pandas as pd

from transformers.hf_argparser import HfArgumentParser

from procyon.data.data_utils import DATA_DIR

prompt_template = """Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. Your evaluation should consider only correctness, and ignore stylistic differences. You will be given a series of reference answers, assistant A's answer, and assistant B's answer. Your job is to evaluate which assistant's answer is most consistent with the reference answers.

Begin your evaluation by briefly comparing both assistants' answers with the reference answer. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. Some responses may be truncated, please ignore this. Please make sure your explanation is as succinct as possible.

After providing your explanation, output your final verdict by strictly following this format: "[[A]]" if assistant A is better, "[[B]]"
if assistant B is better, and "[[C]]" for a tie.

[User Question]
{question}

Start Reference Answer:
{answer_ref}
End Reference Answer

Start Assistant A answer
{answer_a}
End Assistant A answer

Start Assistant B answer
{answer_b}
End Assistant B answer"""

UNIPROT_IDS = pd.read_pickle(
    os.path.join(
        DATA_DIR,
        "integrated_data",
        "v1",
        "protein",
        "protein_info_filtered.pkl",
    )
)[["index", "protein_id"]]

uniprot_annotations = pd.read_table(
    os.path.join(
        DATA_DIR,
        "experimental_data",
        "llm_judge_eval",
        "selected_caption_samples",
        "uniprotkb_proteome_UP000005640_2024_06_18.tsv.gz",
    )
)
up_gene_name_map = {row[0]:row[1] for row in uniprot_annotations[["Entry", "Gene Names"]].itertuples(index=False)}

def uniprot_id_to_index(uniprot_id):
    assert (UNIPROT_IDS["protein_id"] == uniprot_id).sum() == 1, "ID {} not found in internal database".format(uniprot_id)
    i = UNIPROT_IDS["index"].loc[UNIPROT_IDS["protein_id"] == uniprot_id].item()
    return i

def index_to_uniprot_id(i):
    uniprot_id = UNIPROT_IDS["protein_id"].loc[UNIPROT_IDS["index"] == i].item()
    return uniprot_id

def captions_to_markdown(
    captions: List[str],
    protein_id: str,
) -> str:
    # Preamble added to match external LLM's propensity to repeat back the gene name at the
    # beginning of the response.
    resp = f"Here's a description for the protein {up_gene_name_map[protein_id].split()[0]}:\n"

    # Note that we empirically found that providing a single phenotype description was more
    # well-received than providing multiple.
    for ans in captions[:1]:
        resp += f"- {ans}\n"
    return resp

def load_procyon_captions(
    path: str,
) -> pd.DataFrame:
    captions = pd.read_table(path, compression="gzip")
    dedup = (
        captions
        [["seq_id", "generated_caption", "reference_caption"]]
        .assign(
            protein_id=lambda x: x["seq_id"].apply(index_to_uniprot_id)
        )
    )

    rows = []
    for protein_id, df in dedup.groupby("protein_id"):
        rows.append({
            "protein_id": protein_id,
            "reference_captions_list": df.reference_caption.drop_duplicates().to_list(),
            "generated_caption": captions_to_markdown(df.generated_caption.drop_duplicates().to_list(), protein_id),
        })
    return pd.DataFrame(rows)

def load_llm_captions(
    path: str,
) -> pd.DataFrame:
    return (
        pd.read_csv(path)
        .rename(columns={
            "Protein ID": "protein_id",
            "response": "generated_caption",
        })
        [["protein_id", "generated_caption", "prompt"]]
    )

def construct_paired_prompts(
    procyon_caption: str,
    llm_caption: str,
    reference_captions: List[str],
    prompt: str,
) -> Tuple[str]:
    reference_str = ""
    for i, ref in enumerate(reference_captions):
        reference_str += (f"Reference answer {i}:\n")
        reference_str += ref + "\n"
    return (prompt_template.format(
        question=prompt,
        answer_ref=reference_str,
        answer_a=procyon_caption,
        answer_b=llm_caption
    ), prompt_template.format(
        question=prompt,
        answer_ref=reference_str,
        answer_a=llm_caption,
        answer_b=procyon_caption
    ))


@dataclass
class ScriptArgs:
    procyon_phenotypes_path: str = field(
        default=None,
        metadata={
            "help": "Path to ProCyon phenotype outputs TSV, as generated by the caption evaluation framework. "
                    "Expected to have the following columns: `seq_id`, `generated_caption`, `reference_caption`. "
                    "Proteins with multiple reference phenotypes should be represented by multiple rows."
        },
    )
    llm_phenotypes_path: str = field(
        default=None,
        metadata={
            "help": "Path to external LLM phenotype outputs as CSV."
                    "Expected to have the following columns: `Protein ID`, `prompt`, `response`. "
        },
    )
    output_path: str = field(
        default=None,
        metadata={
            "help": "Path to write generated prompts."
        },
    )

def merge_and_write(
        procyon_captions: pd.DataFrame,
        llm_captions: pd.DataFrame,
        outpath: str,
):
    merged_captions = procyon_captions.merge(
        llm_captions,
        on="protein_id",
        suffixes=("_procyon", "_llm"),
    )
    prompt_df = defaultdict(list)
    for row in merged_captions.itertuples(index=False):
        prompt_a, prompt_b = construct_paired_prompts(
            row.generated_caption_procyon,
            row.generated_caption_llm,
            row.reference_captions_list,
            row.prompt,
        )
        prompt_df["Protein ID"].extend([row.protein_id, row.protein_id])
        prompt_df["Prompt"].extend([prompt_a, prompt_b])
        prompt_df["prompt_a"].extend(["procyon", "llm"])
        prompt_df["prompt_b"].extend(["llm", "procyon"])

    with open(outpath, "w") as fh:
        pd.DataFrame(prompt_df).to_csv(fh, index=False)


def generate_comparison_prompts(
    args: ScriptArgs,
):
    procyon_captions = load_procyon_captions(args.procyon_phenotypes_path)
    llm_captions_gene = load_llm_captions(args.llm_phenotypes_path)

    merge_and_write(
        procyon_captions,
        llm_captions_gene,
        args.output_path,
    )

if __name__ == '__main__':
    parser = HfArgumentParser(ScriptArgs)
    (args,) = parser.parse_args_into_dataclasses()
    generate_comparison_prompts(args)
