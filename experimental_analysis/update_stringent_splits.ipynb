{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from procyon.data.data_utils import DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from procyon.data.constants import CANONICAL_SPLITS, RETRIEVAL_SUBSETS\n",
    "from procyon.data.data_utils import DATA_DIR\n",
    "\n",
    "data_path = os.path.join(\n",
    "    DATA_DIR,\n",
    "    \"integrated_data\",\n",
    "    \"v1\",\n",
    ")\n",
    "\n",
    "all_datasets = [\n",
    "    (\"domain\", \"go\"),\n",
    "    (\"domain\", \"pfam\"),\n",
    "    (\"protein\", \"disgenet\"),\n",
    "    (\"protein\", \"drugbank\"),\n",
    "    (\"protein\", \"ec\"),\n",
    "    (\"protein\", \"go\"),\n",
    "    (\"protein\", \"gtop\"),\n",
    "    (\"protein\", \"omim\"),\n",
    "    (\"protein\", \"reactome\"),\n",
    "    (\"protein\", \"uniprot\"),\n",
    "]\n",
    "\n",
    "test_datasets = [\n",
    "    x for x in all_datasets if x[1] not in [\"uniprot\", \"gtop\"]\n",
    "]\n",
    "\n",
    "def load_rels(aaseq_type: str, name: str) -> pd.DataFrame:\n",
    "    \"\"\"Get phenotype-protein relations for a given dataset\"\"\"\n",
    "    split_name = CANONICAL_SPLITS[name]\n",
    "    rels_path = os.path.join(\n",
    "        data_path,\n",
    "        f\"{aaseq_type}_{name}\",\n",
    "        split_name,\n",
    "        f\"{aaseq_type}_{name}_relations.unified.csv\"\n",
    "    )\n",
    "    return pd.read_csv(rels_path)\n",
    "\n",
    "\n",
    "def get_train_relations():\n",
    "    \"\"\"Get training relations for all datasets\n",
    "\n",
    "    Returns a DF with:\n",
    "     - dataset_idx - numeric idx within the dataset\n",
    "     - dataset - the dataset name\n",
    "     - unique_id - concatenation of `dataset` and `dataset_idx`, unique identifier\n",
    "                   across all datasets\n",
    "     - seq_id - set containing the protein IDs related to this phenotype\n",
    "     - count - number of proteins related to this phenotype, i.e. length of `seq_id`\n",
    "    \"\"\"\n",
    "    all_rels = []\n",
    "    for dset in all_datasets:\n",
    "        all_rels.append((\n",
    "            load_rels(*dset)\n",
    "            .query(\"split == 'CL_train'\")\n",
    "            .groupby(\"text_id\")\n",
    "            .seq_id\n",
    "            .apply(set)\n",
    "            .reset_index()\n",
    "            .assign(dataset=dset, count=lambda x: x.apply(lambda y: len(y.seq_id), axis=1))\n",
    "            .rename(columns={\"text_id\": \"dataset_idx\"})\n",
    "        ))\n",
    "    return pd.concat(all_rels).assign(unique_id=lambda x: x.dataset + \"_\" + x.dataset_idx.astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exact term mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper_dir = Path(DATA_DIR) / \"experimental_data\" / \"db_mappers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_330564/2686004306.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pd.read_csv(\n",
      "/tmp/ipykernel_330564/2686004306.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "def load_mapper(db_name: str) -> pd.DataFrame:\n",
    "    mapper = (\n",
    "        pd.read_csv(\n",
    "            mapper_dir / f\"{db_name}2go.txt\",\n",
    "            sep=\" > \",\n",
    "            comment=\"!\",\n",
    "            names=[f\"{db_name}_id\", \"oth\"],\n",
    "        )\n",
    "        .assign(\n",
    "            go_id=lambda x: x.oth.str.split(\";\").str[-1].str.strip(),\n",
    "        )\n",
    "        .drop(columns=[\"oth\"])\n",
    "    )\n",
    "    mapper[f\"{db_name}_id\"] = (\n",
    "        mapper[f\"{db_name}_id\"]\n",
    "        .str.split()\n",
    "        .str[0]\n",
    "        .str.split(\":\")\n",
    "        .str[1]\n",
    "    )\n",
    "    return mapper\n",
    "\n",
    "orig_mappers = {\n",
    "    \"ec\": load_mapper(\"ec\"),\n",
    "    \"pfam\": load_mapper(\"pfam\"),\n",
    "    \"reactome\": (\n",
    "        pd.concat([\n",
    "            pd.read_table(mapper_dir / \"Pathways2GoTerms_human.txt\"),\n",
    "            pd.read_table(mapper_dir / \"Reactions2GoTerms_human.txt\"),\n",
    "        ])\n",
    "        .rename(columns={\"Identifier\":\"reactome_id\", \"GO_Term\":\"go_id\"})\n",
    "        .drop(columns=[\"Name\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec <-> GO: 5236 exact mappings\n",
      "pfam <-> GO: 10095 exact mappings\n",
      "reactome <-> GO: 7363 exact mappings\n",
      "ec <-> pfam: 3829 exact mappings\n",
      "ec <-> reactome: 16233 exact mappings\n",
      "pfam <-> reactome: 36090 exact mappings\n"
     ]
    }
   ],
   "source": [
    "orig_dest_dbs = list(orig_mappers.keys())\n",
    "\n",
    "# Mappers is a dict of `src` to {dest_x: df, dest_y: df,...} mapping a DB name\n",
    "# to a dict of {dest: df} where `df`` has columns `src_id` and `dest_id`\n",
    "mappers = defaultdict(dict)\n",
    "for db in orig_dest_dbs:\n",
    "    mappers[\"go\"][db] = orig_mappers[db]\n",
    "    mappers[db][\"go\"] = orig_mappers[db]\n",
    "    print(f\"{db} <-> GO: {len(orig_mappers[db])} exact mappings\")\n",
    "\n",
    "for db_a, db_b in combinations(orig_dest_dbs, r=2):\n",
    "    mapper_a = orig_mappers[db_a]\n",
    "    mapper_b = orig_mappers[db_b]\n",
    "    trans_mapper = mapper_a.merge(mapper_b, on=\"go_id\").drop(columns=\"go_id\")\n",
    "    mappers[db_a][db_b] = trans_mapper\n",
    "    mappers[db_b][db_a] = trans_mapper\n",
    "    print(f\"{db_a} <-> {db_b}: {len(trans_mapper)} exact mappings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count\n",
       "1      921\n",
       "2      460\n",
       "3      289\n",
       "4      156\n",
       "5       70\n",
       "      ... \n",
       "310      4\n",
       "385      1\n",
       "509     10\n",
       "510      7\n",
       "532      1\n",
       "Name: count, Length: 72, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappers[\"pfam\"][\"reactome\"].pfam_id.value_counts().value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pfam_id\n",
       "PF09036    532\n",
       "PF09202    510\n",
       "PF08826    510\n",
       "PF15785    510\n",
       "PF00433    510\n",
       "          ... \n",
       "PF19052      1\n",
       "PF19088      1\n",
       "PF00023      1\n",
       "PF05937      1\n",
       "PF05955      1\n",
       "Name: count, Length: 2552, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappers[\"pfam\"][\"reactome\"].pfam_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pfam_id</th>\n",
       "      <th>reactome_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24764</th>\n",
       "      <td>PF09036</td>\n",
       "      <td>R-HSA-109702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24765</th>\n",
       "      <td>PF09036</td>\n",
       "      <td>R-HSA-109822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24766</th>\n",
       "      <td>PF09036</td>\n",
       "      <td>R-HSA-109823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24767</th>\n",
       "      <td>PF09036</td>\n",
       "      <td>R-HSA-109860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24768</th>\n",
       "      <td>PF09036</td>\n",
       "      <td>R-HSA-109862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25291</th>\n",
       "      <td>PF09036</td>\n",
       "      <td>R-HSA-9018745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25292</th>\n",
       "      <td>PF09036</td>\n",
       "      <td>R-HSA-9018806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25293</th>\n",
       "      <td>PF09036</td>\n",
       "      <td>R-HSA-9624893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25294</th>\n",
       "      <td>PF09036</td>\n",
       "      <td>R-HSA-9693282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25295</th>\n",
       "      <td>PF09036</td>\n",
       "      <td>R-HSA-212718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>532 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pfam_id    reactome_id\n",
       "24764  PF09036   R-HSA-109702\n",
       "24765  PF09036   R-HSA-109822\n",
       "24766  PF09036   R-HSA-109823\n",
       "24767  PF09036   R-HSA-109860\n",
       "24768  PF09036   R-HSA-109862\n",
       "...        ...            ...\n",
       "25291  PF09036  R-HSA-9018745\n",
       "25292  PF09036  R-HSA-9018806\n",
       "25293  PF09036  R-HSA-9624893\n",
       "25294  PF09036  R-HSA-9693282\n",
       "25295  PF09036   R-HSA-212718\n",
       "\n",
       "[532 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappers[\"pfam\"][\"reactome\"].query(\"pfam_id == 'PF09036'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query_db_id</th>\n",
       "      <th>query_text_col</th>\n",
       "      <th>max_train_sim</th>\n",
       "      <th>max_train_sim_count</th>\n",
       "      <th>num_matched_seqs</th>\n",
       "      <th>matched_text_count</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>GO:0000152</td>\n",
       "      <td>go_def</td>\n",
       "      <td>0.624915</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>360</td>\n",
       "      <td>GO:0004143</td>\n",
       "      <td>go_def</td>\n",
       "      <td>0.627025</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>361</td>\n",
       "      <td>GO:0004175</td>\n",
       "      <td>go_def</td>\n",
       "      <td>0.611923</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>363</td>\n",
       "      <td>GO:0004180</td>\n",
       "      <td>go_def</td>\n",
       "      <td>0.646375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>365</td>\n",
       "      <td>GO:0004190</td>\n",
       "      <td>go_def</td>\n",
       "      <td>0.612427</td>\n",
       "      <td>15.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id query_db_id query_text_col  max_train_sim  max_train_sim_count  \\\n",
       "0        25  GO:0000152         go_def       0.624915                  1.0   \n",
       "1       360  GO:0004143         go_def       0.627025                  1.6   \n",
       "2       361  GO:0004175         go_def       0.611923                  1.0   \n",
       "3       363  GO:0004180         go_def       0.646375                  1.0   \n",
       "4       365  GO:0004190         go_def       0.612427                 15.2   \n",
       "\n",
       "   num_matched_seqs  matched_text_count dataset  \n",
       "0                 0                   0      go  \n",
       "1                 0                   0      go  \n",
       "2                 0                   0      go  \n",
       "3                 0                   0      go  \n",
       "4                 0                   0      go  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sims_path = Path(DATA_DIR) / \"experimental_data\" / \"phenotype_embeddings\" / \"eval_phenotype_sims.tsv.gz\"\n",
    "eval_sims = pd.read_table(eval_sims_path)\n",
    "eval_sims.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count\n",
       "1    1531\n",
       "4    1224\n",
       "2    1058\n",
       "3      75\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some phenotypes are associated with multiple texts (e.g. drugbank MoA and indication)\n",
    "eval_sims.query_db_id.value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_db_id</th>\n",
       "      <th>max_train_sim</th>\n",
       "      <th>num_matched_seqs</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1.1.104</td>\n",
       "      <td>0.588074</td>\n",
       "      <td>1</td>\n",
       "      <td>ec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.1.1.153</td>\n",
       "      <td>0.654462</td>\n",
       "      <td>1</td>\n",
       "      <td>ec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.1.1.159</td>\n",
       "      <td>0.672711</td>\n",
       "      <td>1</td>\n",
       "      <td>ec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.1.1.170</td>\n",
       "      <td>0.728780</td>\n",
       "      <td>0</td>\n",
       "      <td>ec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.1.1.189</td>\n",
       "      <td>0.688617</td>\n",
       "      <td>1</td>\n",
       "      <td>ec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_db_id  max_train_sim  num_matched_seqs dataset\n",
       "0   1.1.1.104       0.588074                 1      ec\n",
       "1   1.1.1.153       0.654462                 1      ec\n",
       "2   1.1.1.159       0.672711                 1      ec\n",
       "3   1.1.1.170       0.728780                 0      ec\n",
       "4   1.1.1.189       0.688617                 1      ec"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unique_and_check(x):\n",
    "    uniq = np.unique(x)\n",
    "    assert len(uniq) == 1, f\"got: {uniq}\"\n",
    "    return uniq[0]\n",
    "\n",
    "eval_sims = (\n",
    "    eval_sims\n",
    "    .drop(columns=[\"query_id\", \"query_text_col\", \"matched_text_count\", \"max_train_sim_count\"])\n",
    "    .groupby(\"query_db_id\")\n",
    "    .agg({\n",
    "        \"max_train_sim\": \"mean\",\n",
    "        \"num_matched_seqs\": \"max\",\n",
    "        \"dataset\": unique_and_check,\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "eval_sims.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3888"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaseq_map = {\n",
    "    \"pfam\": [\"domain\"],\n",
    "    \"reactome\": [\"protein\"],\n",
    "    \"ec\": [\"protein\"],\n",
    "    \"go\": [\"domain\", \"protein\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_from_exact_matches(\n",
    "    rels: pd.DataFrame,\n",
    "    dataset: str,\n",
    ") -> pd.DataFrame:\n",
    "    if dataset not in mappers:\n",
    "        return rels\n",
    "\n",
    "    for dest_db, map_df in mappers[dataset].items():\n",
    "        for oth_aaseq in aaseq_map[dest_db]:\n",
    "            oth_rels = (\n",
    "                load_rels(oth_aaseq, dest_db)\n",
    "                .assign(**{\n",
    "                    f\"{dest_db}_id\": lambda x: x.text_id,\n",
    "                })\n",
    "            )\n",
    "            has_train_rel = (\n",
    "                rels\n",
    "                .merge(map_df, on=f\"{dataset}_id\")\n",
    "                .merge(\n",
    "                    oth_rels.rename(columns={\"split\": \"oth_split\"})[[f\"{dest_db}_id\", \"oth_split\"]],\n",
    "                    on=f\"{dest_db}_id\",\n",
    "                )\n",
    "                .groupby(f\"{dataset}_id\")\n",
    "                .oth_split\n",
    "                .apply(lambda x: \"CL_train\" in np.unique(x))\n",
    "                [lambda x: x]\n",
    "                .reset_index()\n",
    "                [f\"{dataset}_id\"]\n",
    "            )\n",
    "            rels.loc[lambda x: x[f\"{dataset}_id\"].isin(has_train_rel), \"reason\"] += f\",{dest_db}_train\"\n",
    "    return rels\n",
    "\n",
    "def flag_from_sims(\n",
    "    rels: pd.DataFrame,\n",
    "    dataset: str,\n",
    "    max_sim: float = 0.8,\n",
    "    max_sim_w_match: float = 0.6,\n",
    ") -> pd.DataFrame:\n",
    "    zero_shot_texts = (\n",
    "        rels\n",
    "        .query(\"split == 'eval_zero_shot'\")\n",
    "        .drop_duplicates(f\"{dataset}_id\")\n",
    "    )\n",
    "    want_sims = (\n",
    "        eval_sims\n",
    "        .loc[lambda x: x.dataset == dataset]\n",
    "        .rename(columns={\"query_db_id\": f\"{dataset}_id\"})\n",
    "    )\n",
    "    merged = zero_shot_texts.merge(want_sims, on=f\"{dataset}_id\")\n",
    "\n",
    "    # All zero-shot texts should be in here\n",
    "    assert len(merged) == len(zero_shot_texts)\n",
    "    flagged = (\n",
    "        merged\n",
    "        .loc[lambda x: (x.max_train_sim >= max_sim) | ((x.num_matched_seqs != 0) & (x.max_train_sim >= max_sim_w_match))]\n",
    "        [f\"{dataset}_id\"]\n",
    "    )\n",
    "    rels.loc[lambda x: x[f\"{dataset}_id\"].isin(flagged), \"reason\"] += f\",train_sim\"\n",
    "    return rels\n",
    "\n",
    "def annotate_relations(\n",
    "    aaseq: str,\n",
    "    dataset: str,\n",
    ") -> pd.DataFrame:\n",
    "    all_rels = (\n",
    "        load_rels(aaseq, dataset)\n",
    "        .assign(**{\n",
    "            f\"{dataset}_id\": lambda x: x.text_id.astype(str),\n",
    "            \"reason\": \"\",\n",
    "        })\n",
    "        .pipe(flag_from_exact_matches, dataset=dataset)\n",
    "        .pipe(flag_from_sims, dataset=dataset)\n",
    "        .assign(\n",
    "            reason=lambda x: x.reason.str.strip(\",\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    zero_shot_rels = (\n",
    "        all_rels\n",
    "        # Include eval_zero_shot_hard here just so we can recreate the numbers\n",
    "        # after the first run.\n",
    "        .loc[lambda x: x.split.isin([\"eval_zero_shot\", \"eval_zero_shot_hard\"])]\n",
    "        .assign(flagged=lambda x: x.reason != \"\")\n",
    "    )\n",
    "    print(f\"{aaseq}-{dataset}: {len(zero_shot_rels.query('flagged'))} / {len(zero_shot_rels)} ({len(zero_shot_rels.query('flagged')) / len(zero_shot_rels): .1%}) zero-shot relations are flagged\")\n",
    "\n",
    "    zero_shot_texts = (\n",
    "        zero_shot_rels\n",
    "        .groupby(f\"{dataset}_id\")\n",
    "        .agg({\n",
    "            \"flagged\": \"any\",\n",
    "        })\n",
    "    )\n",
    "    print(f\"{aaseq}-{dataset}: {len(zero_shot_texts.query('flagged'))} / {len(zero_shot_texts)} ({len(zero_shot_texts.query('flagged')) / len(zero_shot_texts):.1%}) zero-shot texts are flagged\")\n",
    "    return all_rels.drop(columns=f\"{dataset}_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain-go: 1861 / 3086 ( 60.3%) zero-shot relations are flagged\n",
      "domain-go: 33 / 70 (47.1%) zero-shot texts are flagged\n",
      "domain-pfam: 303 / 1036 ( 29.2%) zero-shot relations are flagged\n",
      "domain-pfam: 183 / 638 (28.7%) zero-shot texts are flagged\n",
      "protein-disgenet: 208 / 406 ( 51.2%) zero-shot relations are flagged\n",
      "protein-disgenet: 69 / 179 (38.5%) zero-shot texts are flagged\n",
      "protein-drugbank: 202 / 1589 ( 12.7%) zero-shot relations are flagged\n",
      "protein-drugbank: 27 / 269 (10.0%) zero-shot texts are flagged\n",
      "protein-ec: 107 / 145 ( 73.8%) zero-shot relations are flagged\n",
      "protein-ec: 46 / 70 (65.7%) zero-shot texts are flagged\n",
      "protein-go: 5956 / 12073 ( 49.3%) zero-shot relations are flagged\n",
      "protein-go: 153 / 274 (55.8%) zero-shot texts are flagged\n",
      "protein-omim: 306 / 645 ( 47.4%) zero-shot relations are flagged\n",
      "protein-omim: 294 / 623 (47.2%) zero-shot texts are flagged\n",
      "protein-reactome: 355 / 506 ( 70.2%) zero-shot relations are flagged\n",
      "protein-reactome: 182 / 258 (70.5%) zero-shot texts are flagged\n"
     ]
    }
   ],
   "source": [
    "all_annotated = []\n",
    "for aaseq, dataset in test_datasets:\n",
    "    split_name  = CANONICAL_SPLITS[dataset]\n",
    "    rels_path = os.path.join(\n",
    "        data_path,\n",
    "        f\"{aaseq}_{dataset}\",\n",
    "        split_name,\n",
    "        f\"{aaseq}_{dataset}_relations.unified.csv\"\n",
    "    )\n",
    "\n",
    "    rels_indexed_path = os.path.join(\n",
    "        data_path,\n",
    "        f\"{aaseq}_{dataset}\",\n",
    "        split_name,\n",
    "        f\"{aaseq}_{dataset}_relations_indexed.unified.csv\"\n",
    "    )\n",
    "    rels_indexed = pd.read_csv(rels_indexed_path)\n",
    "\n",
    "    text_df = pd.read_pickle(os.path.join(\n",
    "        data_path,\n",
    "        dataset,\n",
    "        f\"{dataset}_info_filtered_composed.pkl\"\n",
    "    ))\n",
    "\n",
    "    seq_to_index = {}\n",
    "    for index, text_id in text_df[[\"index\", f\"{dataset}_id\"]].itertuples(index=False):\n",
    "        seq_to_index[text_id] = index\n",
    "\n",
    "    annotated = (\n",
    "        annotate_relations(aaseq, dataset)\n",
    "        .assign(text_idx=lambda x: x.text_id.map(seq_to_index))\n",
    "    )\n",
    "\n",
    "    new_rels = (\n",
    "        annotated\n",
    "        .assign(\n",
    "            split=lambda x: np.where(\n",
    "                x.split == \"eval_zero_shot\",\n",
    "                np.where(\n",
    "                    x.reason != \"\",\n",
    "                    \"eval_zero_shot\",\n",
    "                    \"eval_zero_shot_hard\",\n",
    "                ),\n",
    "                x.split,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    old_len = len(rels_indexed)\n",
    "    new_rels_indexed = (\n",
    "        rels_indexed\n",
    "        .merge(new_rels[[\"reason\", \"text_idx\"]].rename(columns={\"text_idx\": \"text_id\"}).drop_duplicates(), on=\"text_id\")\n",
    "        .assign(\n",
    "            split=lambda x: np.where(\n",
    "                x.split == \"eval_zero_shot\",\n",
    "                np.where(\n",
    "                    x.reason != \"\",\n",
    "                    \"eval_zero_shot\",\n",
    "                    \"eval_zero_shot_hard\",\n",
    "                ),\n",
    "                x.split,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    assert len(new_rels_indexed) == old_len\n",
    "\n",
    "    (\n",
    "        new_rels\n",
    "        .drop(columns=[\"reason\", \"text_idx\"])\n",
    "        .to_csv(rels_path, index=False)\n",
    "    )\n",
    "\n",
    "    (\n",
    "        new_rels_indexed\n",
    "        .drop(columns=[\"reason\"])\n",
    "        .to_csv(rels_indexed_path, index=False)\n",
    "    )\n",
    "    all_annotated.append(annotated)\n",
    "\n",
    "all_annotated = pd.concat(all_annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 9298 / 19486 ( 47.7%) zero-shot relations are flagged\n",
      "total: 987 / 2381 (41.5%) zero-shot texts are flagged\n"
     ]
    }
   ],
   "source": [
    "zero_shot_rels = (\n",
    "    all_annotated\n",
    "    .loc[lambda x: x.split.isin([\"eval_zero_shot\", \"eval_zero_shot_hard\"])]\n",
    "    .assign(\n",
    "        flagged=lambda x: x.reason != \"\",\n",
    "        flagged_sim=lambda x: x.reason.str.contains(\"train_sim\") & ~x.reason.str.contains(\"_train\"),\n",
    "        flagged_map=lambda x: ~x.reason.str.contains(\"train_sim\") & x.reason.str.contains(\"_train\"),\n",
    "    )\n",
    ")\n",
    "print(f\"total: {len(zero_shot_rels.query('flagged'))} / {len(zero_shot_rels)} ({len(zero_shot_rels.query('flagged')) / len(zero_shot_rels): .1%}) zero-shot relations are flagged\")\n",
    "\n",
    "zero_shot_texts = (\n",
    "    zero_shot_rels\n",
    "    .assign(unique_id=lambda x: x.text_type.astype(str)+\"_\"+x.text_id.astype(str))\n",
    "    .groupby(\"unique_id\")\n",
    "    .agg({\n",
    "        \"flagged\": \"any\",\n",
    "        \"flagged_sim\": \"any\",\n",
    "        \"flagged_map\": \"any\",\n",
    "    })\n",
    ")\n",
    "print(f\"total: {len(zero_shot_texts.query('flagged'))} / {len(zero_shot_texts)} ({len(zero_shot_texts.query('flagged')) / len(zero_shot_texts):.1%}) zero-shot texts are flagged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flagged_map\n",
       "False    2145\n",
       "True      236\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_texts.flagged_map.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flagged_sim\n",
       "False    1698\n",
       "True      683\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_texts.flagged_sim.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    1698\n",
       "True      683\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(zero_shot_texts.flagged_sim & ~zero_shot_texts.flagged_map).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flagged\n",
       "False    1394\n",
       "True      987\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_texts.flagged.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
